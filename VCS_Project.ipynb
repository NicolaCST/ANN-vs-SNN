{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["YqtFWXfG2Ekl","iWJGBXDs2Ish","IfzpK0gl2TJ4","sPraYV81ZdXc","Jl3K9ujaZftc","wImeKmPU3ASG","NSffkyLU4R24","3feFcA2c6vj6","IDWpn6rx5cgr","_ZgGQy3fPGoY","G3xaheLiPxWf","bxh7Vgst6F66"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"9990778e40dc477eb16a02fc454b642e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0cee7595271464a958702a0410ff32f","IPY_MODEL_42373dc2764140aa8e3928aa04ff66d7","IPY_MODEL_60f90ca8ed7e49249c7c0cc4f37cf4ca"],"layout":"IPY_MODEL_9a7eb5e671694921a0a3135959d717a1"}},"b0cee7595271464a958702a0410ff32f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1128ee2e0b84377acda4bc56a1b76fa","placeholder":"​","style":"IPY_MODEL_bce48006fa834c378594785bffb64341","value":"100%"}},"42373dc2764140aa8e3928aa04ff66d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51c24de1000942c2a350697f587d59b9","max":102540417,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e99086bba46349469d623d5142d9d74f","value":102540417}},"60f90ca8ed7e49249c7c0cc4f37cf4ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f8108a1fa8c45169bf296853751fbfe","placeholder":"​","style":"IPY_MODEL_31b2ae532553487bb03b1a7794b4f8ca","value":" 97.8M/97.8M [00:01&lt;00:00, 69.7MB/s]"}},"9a7eb5e671694921a0a3135959d717a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1128ee2e0b84377acda4bc56a1b76fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bce48006fa834c378594785bffb64341":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51c24de1000942c2a350697f587d59b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e99086bba46349469d623d5142d9d74f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f8108a1fa8c45169bf296853751fbfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31b2ae532553487bb03b1a7794b4f8ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebf8dce3218f42759fd58f66a390daee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_573014846e2b44bcbc9942c31039acb5","IPY_MODEL_5cb2f1d6e3b7474aab0e71ffe7c5841b","IPY_MODEL_2bc3f0f20409417e80880ccb951eb4fe"],"layout":"IPY_MODEL_725be2034d5e482f888e3165ceab0a49"}},"573014846e2b44bcbc9942c31039acb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af2c352486b843b089f406e3319ff22d","placeholder":"​","style":"IPY_MODEL_292e4e9ef5c349429ec01c56385623d8","value":"100%"}},"5cb2f1d6e3b7474aab0e71ffe7c5841b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25ff4b407ab3490bb0c642979c4072c7","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5c06f4a217c4413bf9679d89b1996fe","value":170498071}},"2bc3f0f20409417e80880ccb951eb4fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5fe04abcea04e34936dd57bccf31590","placeholder":"​","style":"IPY_MODEL_e2240e934e644c7d94429332f8fbd69f","value":" 170498071/170498071 [00:02&lt;00:00, 76693278.63it/s]"}},"725be2034d5e482f888e3165ceab0a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af2c352486b843b089f406e3319ff22d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"292e4e9ef5c349429ec01c56385623d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25ff4b407ab3490bb0c642979c4072c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5c06f4a217c4413bf9679d89b1996fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5fe04abcea04e34936dd57bccf31590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2240e934e644c7d94429332f8fbd69f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69a5db5404c543d8afd5d820ce43d94d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c1919534bd94519b07371b876b563d5","IPY_MODEL_5207dc8de52e414b87e2c68d7c6ddbec","IPY_MODEL_a6e6790a424a4a26a9cbaa8f07c83650"],"layout":"IPY_MODEL_7d323832f25441eaa0aef3a7754473f7"}},"0c1919534bd94519b07371b876b563d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a11302aefb845f99b5fed100c6708a2","placeholder":"​","style":"IPY_MODEL_c9f6c0efcae446b98241c93eab1f11ae","value":"100%"}},"5207dc8de52e414b87e2c68d7c6ddbec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_19f191415c0a437b9094d261125b423e","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_891ad5174dc0486da93a36736a64f9a5","value":170498071}},"a6e6790a424a4a26a9cbaa8f07c83650":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_987c285e80ae45e6beb10332055f1bbf","placeholder":"​","style":"IPY_MODEL_fcc5368331424993a4c2f6fa2d38018b","value":" 170498071/170498071 [00:02&lt;00:00, 77148856.60it/s]"}},"7d323832f25441eaa0aef3a7754473f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a11302aefb845f99b5fed100c6708a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f6c0efcae446b98241c93eab1f11ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19f191415c0a437b9094d261125b423e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"891ad5174dc0486da93a36736a64f9a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"987c285e80ae45e6beb10332055f1bbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcc5368331424993a4c2f6fa2d38018b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"YqtFWXfG2Ekl"}},{"cell_type":"markdown","source":["###Imports"],"metadata":{"id":"iWJGBXDs2Ish"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHOjbWyC1617","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672250737144,"user_tz":-60,"elapsed":23869,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"f3a6662c-4cf9-4419-f8d9-8115632495a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting codecarbon\n","  Downloading codecarbon-2.1.4-py3-none-any.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 5.6 MB/s \n","\u001b[?25hCollecting py-cpuinfo\n","  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n","Collecting arrow\n","  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n","\u001b[K     |████████████████████████████████| 66 kB 5.5 MB/s \n","\u001b[?25hCollecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from codecarbon) (7.1.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from codecarbon) (5.4.8)\n","Collecting pynvml\n","  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from codecarbon) (1.3.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from codecarbon) (2.23.0)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from arrow->codecarbon) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->codecarbon) (2022.6)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas->codecarbon) (1.21.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->codecarbon) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->codecarbon) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->codecarbon) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->codecarbon) (1.24.3)\n","Installing collected packages: pynvml, py-cpuinfo, fuzzywuzzy, arrow, codecarbon\n","Successfully installed arrow-1.2.3 codecarbon-2.1.4 fuzzywuzzy-0.18.0 py-cpuinfo-9.0.0 pynvml-11.4.1\n"]}],"source":["import time as timing\n","import PIL\n","from PIL import Image\n","from google.colab import drive\n","import pandas as pd\n","import scipy\n","import os\n","import cv2\n","import pickle \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","import math\n","from pathlib import Path\n","import shutil\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.models as tv_models\n","import torch.utils.data as data\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.cuda import amp\n","import torch.nn.functional as F\n","from torchsummary import summary\n","\n","!pip install tqdm\n","from tqdm import tqdm as load_bar\n","\n","!pip install codecarbon\n","import codecarbon"]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  import cupy\n","  device = torch.device(\"cuda\")\n","  name = torch.cuda.get_device_name(0)\n","  device = 'cuda:0'\n","  print(device, name)\n","else:\n","  device = \"cpu\"\n","  print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nglfVhhp2QzF","executionInfo":{"status":"ok","timestamp":1672250739045,"user_tz":-60,"elapsed":1907,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"4c75493d-d6ae-4cb5-b5a6-e928bc92df1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0 Tesla T4\n"]}]},{"cell_type":"code","source":["########################### LATEST VERSION INSTALL ###########################\n","\n","#Currently working on the latest release - Not yet on PyPl, must clone the repo\n","!git clone https://github.com/fangwei123456/spikingjelly.git\n","\n","#Add to the PATH the new directory\n","import sys\n","sys.path.append('/content/spikingjelly/')\n","\n","#Install requirements for spikingjelly\n","%cd /content/spikingjelly/\n","!python setup.py install\n","\n","#Now we can import these modules\n","import spikingjelly\n","from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer, learning\n","from spikingjelly import visualizing\n","from spikingjelly.activation_based import ann2snn\n","from spikingjelly.activation_based.model import spiking_vgg, spiking_resnet\n"],"metadata":{"id":"scDHy3eI2Z4o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672250744694,"user_tz":-60,"elapsed":5653,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"451d72cd-ddab-4156-b7c4-88d030ac8511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'spikingjelly'...\n","remote: Enumerating objects: 12830, done.\u001b[K\n","remote: Counting objects: 100% (2649/2649), done.\u001b[K\n","remote: Compressing objects: 100% (811/811), done.\u001b[K\n","remote: Total 12830 (delta 1924), reused 2501 (delta 1826), pack-reused 10181\u001b[K\n","Receiving objects: 100% (12830/12830), 40.86 MiB | 30.25 MiB/s, done.\n","Resolving deltas: 100% (8526/8526), done.\n","/content/spikingjelly\n","running install\n","running bdist_egg\n","running egg_info\n","creating spikingjelly.egg-info\n","writing spikingjelly.egg-info/PKG-INFO\n","writing dependency_links to spikingjelly.egg-info/dependency_links.txt\n","writing requirements to spikingjelly.egg-info/requires.txt\n","writing top-level names to spikingjelly.egg-info/top_level.txt\n","writing manifest file 'spikingjelly.egg-info/SOURCES.txt'\n","adding license file 'LICENSE'\n","adding license file 'LICENSE-CN'\n","writing manifest file 'spikingjelly.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build\n","creating build/lib\n","creating build/lib/spikingjelly\n","copying spikingjelly/__init__.py -> build/lib/spikingjelly\n","copying spikingjelly/configure.py -> build/lib/spikingjelly\n","creating build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/to_x_rep.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/asl_dvs.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/n_caltech101.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/speechcommands.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/__init__.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/n_mnist.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/shd.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/hardvs.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/nav_gesture.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/dvs128_gesture.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/es_imagenet.py -> build/lib/spikingjelly/datasets\n","copying spikingjelly/datasets/cifar10_dvs.py -> build/lib/spikingjelly/datasets\n","creating build/lib/spikingjelly/visualizing\n","copying spikingjelly/visualizing/__init__.py -> build/lib/spikingjelly/visualizing\n","creating build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/base.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/quantize.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/surrogate.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/encoding.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/layer.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/lynxi_exchange.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/tensor_cache.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/__init__.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/functional.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/spike_op.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/cuda_utils.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/neuron_kernel.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/rnn.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/monitor.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/learning.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/neuron.py -> build/lib/spikingjelly/activation_based\n","copying spikingjelly/activation_based/lava_exchange.py -> build/lib/spikingjelly/activation_based\n","creating build/lib/spikingjelly/timing_based\n","copying spikingjelly/timing_based/encoding.py -> build/lib/spikingjelly/timing_based\n","copying spikingjelly/timing_based/__init__.py -> build/lib/spikingjelly/timing_based\n","copying spikingjelly/timing_based/neuron.py -> build/lib/spikingjelly/timing_based\n","creating build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/PPO.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/mstdp.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/lynxi_fmnist_inference.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/conv_fashion_mnist.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/spiking_lstm_sequential_mnist.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/lif_fc_mnist.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/Spiking_DQN_state.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/cifar10_r11_enabling_spikebased_backpropagation.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/speechcommands.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/A2C.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/__init__.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/DQN_state.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/rsnn_sequential_fmnist.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/classify_dvsg.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/stdp_trace.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/Spiking_A2C.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/mstdpet.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/spiking_lstm_text.py -> build/lib/spikingjelly/activation_based/examples\n","copying spikingjelly/activation_based/examples/Spiking_PPO.py -> build/lib/spikingjelly/activation_based/examples\n","creating build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/base.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/example.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/__init__.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/cfunction.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/neuron_kernel.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","copying spikingjelly/activation_based/auto_cuda/generator.py -> build/lib/spikingjelly/activation_based/auto_cuda\n","creating build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/train_classify.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/train_imagenet_example.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/sew_resnet.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/__init__.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/spiking_resnet.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/parametric_lif_net.py -> build/lib/spikingjelly/activation_based/model\n","copying spikingjelly/activation_based/model/spiking_vgg.py -> build/lib/spikingjelly/activation_based/model\n","creating build/lib/spikingjelly/activation_based/ann2snn\n","copying spikingjelly/activation_based/ann2snn/__init__.py -> build/lib/spikingjelly/activation_based/ann2snn\n","copying spikingjelly/activation_based/ann2snn/converter.py -> build/lib/spikingjelly/activation_based/ann2snn\n","copying spikingjelly/activation_based/ann2snn/utils.py -> build/lib/spikingjelly/activation_based/ann2snn\n","copying spikingjelly/activation_based/ann2snn/modules.py -> build/lib/spikingjelly/activation_based/ann2snn\n","creating build/lib/spikingjelly/activation_based/examples/common\n","copying spikingjelly/activation_based/examples/common/multiprocessing_env.py -> build/lib/spikingjelly/activation_based/examples/common\n","copying spikingjelly/activation_based/examples/common/__init__.py -> build/lib/spikingjelly/activation_based/examples/common\n","creating build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","copying spikingjelly/activation_based/model/tv_ref_classify/transforms.py -> build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","copying spikingjelly/activation_based/model/tv_ref_classify/presets.py -> build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","copying spikingjelly/activation_based/model/tv_ref_classify/__init__.py -> build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","copying spikingjelly/activation_based/model/tv_ref_classify/utils.py -> build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","copying spikingjelly/activation_based/model/tv_ref_classify/sampler.py -> build/lib/spikingjelly/activation_based/model/tv_ref_classify\n","creating build/lib/spikingjelly/activation_based/ann2snn/examples\n","copying spikingjelly/activation_based/ann2snn/examples/cnn_mnist.py -> build/lib/spikingjelly/activation_based/ann2snn/examples\n","copying spikingjelly/activation_based/ann2snn/examples/__init__.py -> build/lib/spikingjelly/activation_based/ann2snn/examples\n","copying spikingjelly/activation_based/ann2snn/examples/resnet18_cifar10.py -> build/lib/spikingjelly/activation_based/ann2snn/examples\n","creating build/lib/spikingjelly/timing_based/examples\n","copying spikingjelly/timing_based/examples/tempotron_mnist.py -> build/lib/spikingjelly/timing_based/examples\n","copying spikingjelly/timing_based/examples/__init__.py -> build/lib/spikingjelly/timing_based/examples\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/spikingjelly\n","creating build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/to_x_rep.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/asl_dvs.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/n_caltech101.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/speechcommands.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/n_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/shd.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/hardvs.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/nav_gesture.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/dvs128_gesture.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/es_imagenet.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","copying build/lib/spikingjelly/datasets/cifar10_dvs.py -> build/bdist.linux-x86_64/egg/spikingjelly/datasets\n","creating build/bdist.linux-x86_64/egg/spikingjelly/visualizing\n","copying build/lib/spikingjelly/visualizing/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/visualizing\n","copying build/lib/spikingjelly/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/base.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/quantize.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/PPO.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/mstdp.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/lynxi_fmnist_inference.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/conv_fashion_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/common\n","copying build/lib/spikingjelly/activation_based/examples/common/multiprocessing_env.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/common\n","copying build/lib/spikingjelly/activation_based/examples/common/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/common\n","copying build/lib/spikingjelly/activation_based/examples/spiking_lstm_sequential_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/lif_fc_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/Spiking_DQN_state.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/cifar10_r11_enabling_spikebased_backpropagation.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/speechcommands.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/A2C.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/DQN_state.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/rsnn_sequential_fmnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/classify_dvsg.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/stdp_trace.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/Spiking_A2C.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/mstdpet.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/spiking_lstm_text.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/examples/Spiking_PPO.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples\n","copying build/lib/spikingjelly/activation_based/surrogate.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/encoding.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/layer.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/lynxi_exchange.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/tensor_cache.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/functional.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/spike_op.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/cuda_utils.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/neuron_kernel.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/base.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/example.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/cfunction.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/neuron_kernel.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/auto_cuda/generator.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda\n","copying build/lib/spikingjelly/activation_based/rnn.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/monitor.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/learning.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/train_classify.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/train_imagenet_example.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/sew_resnet.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/tv_ref_classify/transforms.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/tv_ref_classify/presets.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/tv_ref_classify/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/tv_ref_classify/utils.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/tv_ref_classify/sampler.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify\n","copying build/lib/spikingjelly/activation_based/model/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/spiking_resnet.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/parametric_lif_net.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","copying build/lib/spikingjelly/activation_based/model/spiking_vgg.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn\n","creating build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples\n","copying build/lib/spikingjelly/activation_based/ann2snn/examples/cnn_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples\n","copying build/lib/spikingjelly/activation_based/ann2snn/examples/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples\n","copying build/lib/spikingjelly/activation_based/ann2snn/examples/resnet18_cifar10.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples\n","copying build/lib/spikingjelly/activation_based/ann2snn/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn\n","copying build/lib/spikingjelly/activation_based/ann2snn/converter.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn\n","copying build/lib/spikingjelly/activation_based/ann2snn/utils.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn\n","copying build/lib/spikingjelly/activation_based/ann2snn/modules.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn\n","copying build/lib/spikingjelly/activation_based/neuron.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/activation_based/lava_exchange.py -> build/bdist.linux-x86_64/egg/spikingjelly/activation_based\n","copying build/lib/spikingjelly/configure.py -> build/bdist.linux-x86_64/egg/spikingjelly\n","creating build/bdist.linux-x86_64/egg/spikingjelly/timing_based\n","creating build/bdist.linux-x86_64/egg/spikingjelly/timing_based/examples\n","copying build/lib/spikingjelly/timing_based/examples/tempotron_mnist.py -> build/bdist.linux-x86_64/egg/spikingjelly/timing_based/examples\n","copying build/lib/spikingjelly/timing_based/examples/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/timing_based/examples\n","copying build/lib/spikingjelly/timing_based/encoding.py -> build/bdist.linux-x86_64/egg/spikingjelly/timing_based\n","copying build/lib/spikingjelly/timing_based/__init__.py -> build/bdist.linux-x86_64/egg/spikingjelly/timing_based\n","copying build/lib/spikingjelly/timing_based/neuron.py -> build/bdist.linux-x86_64/egg/spikingjelly/timing_based\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/to_x_rep.py to to_x_rep.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/asl_dvs.py to asl_dvs.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/n_caltech101.py to n_caltech101.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/speechcommands.py to speechcommands.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/n_mnist.py to n_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/shd.py to shd.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/hardvs.py to hardvs.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/nav_gesture.py to nav_gesture.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/dvs128_gesture.py to dvs128_gesture.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/es_imagenet.py to es_imagenet.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/datasets/cifar10_dvs.py to cifar10_dvs.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/visualizing/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/base.py to base.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/quantize.py to quantize.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/PPO.py to PPO.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/mstdp.py to mstdp.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/lynxi_fmnist_inference.py to lynxi_fmnist_inference.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/conv_fashion_mnist.py to conv_fashion_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/common/multiprocessing_env.py to multiprocessing_env.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/common/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/spiking_lstm_sequential_mnist.py to spiking_lstm_sequential_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/lif_fc_mnist.py to lif_fc_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/Spiking_DQN_state.py to Spiking_DQN_state.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/cifar10_r11_enabling_spikebased_backpropagation.py to cifar10_r11_enabling_spikebased_backpropagation.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/speechcommands.py to speechcommands.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/A2C.py to A2C.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/DQN_state.py to DQN_state.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/rsnn_sequential_fmnist.py to rsnn_sequential_fmnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/classify_dvsg.py to classify_dvsg.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/stdp_trace.py to stdp_trace.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/Spiking_A2C.py to Spiking_A2C.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/mstdpet.py to mstdpet.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/spiking_lstm_text.py to spiking_lstm_text.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/examples/Spiking_PPO.py to Spiking_PPO.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/surrogate.py to surrogate.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/encoding.py to encoding.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/layer.py to layer.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/lynxi_exchange.py to lynxi_exchange.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/tensor_cache.py to tensor_cache.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/functional.py to functional.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/spike_op.py to spike_op.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/cuda_utils.py to cuda_utils.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/neuron_kernel.py to neuron_kernel.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/base.py to base.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/example.py to example.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/cfunction.py to cfunction.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/neuron_kernel.py to neuron_kernel.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/auto_cuda/generator.py to generator.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/rnn.py to rnn.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/monitor.py to monitor.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/learning.py to learning.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/train_classify.py to train_classify.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/train_imagenet_example.py to train_imagenet_example.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/sew_resnet.py to sew_resnet.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify/transforms.py to transforms.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify/presets.py to presets.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify/utils.py to utils.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/tv_ref_classify/sampler.py to sampler.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/spiking_resnet.py to spiking_resnet.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/parametric_lif_net.py to parametric_lif_net.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/model/spiking_vgg.py to spiking_vgg.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples/cnn_mnist.py to cnn_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/examples/resnet18_cifar10.py to resnet18_cifar10.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/converter.py to converter.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/utils.py to utils.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/ann2snn/modules.py to modules.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/neuron.py to neuron.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/activation_based/lava_exchange.py to lava_exchange.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/configure.py to configure.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/timing_based/examples/tempotron_mnist.py to tempotron_mnist.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/timing_based/examples/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/timing_based/encoding.py to encoding.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/timing_based/__init__.py to __init__.cpython-38.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/spikingjelly/timing_based/neuron.py to neuron.cpython-38.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying spikingjelly.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying spikingjelly.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying spikingjelly.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying spikingjelly.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying spikingjelly.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating dist\n","creating 'dist/spikingjelly-0.0.0.0.13-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing spikingjelly-0.0.0.0.13-py3.8.egg\n","Copying spikingjelly-0.0.0.0.13-py3.8.egg to /usr/local/lib/python3.8/dist-packages\n","Adding spikingjelly 0.0.0.0.13 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.8/dist-packages/spikingjelly-0.0.0.0.13-py3.8.egg\n","Processing dependencies for spikingjelly==0.0.0.0.13\n","Searching for scipy==1.7.3\n","Best match: scipy 1.7.3\n","Adding scipy 1.7.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for torchvision==0.14.0+cu116\n","Best match: torchvision 0.14.0+cu116\n","Adding torchvision 0.14.0+cu116 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for tqdm==4.64.1\n","Best match: tqdm 4.64.1\n","Adding tqdm 4.64.1 to easy-install.pth file\n","Installing tqdm script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for numpy==1.21.6\n","Best match: numpy 1.21.6\n","Adding numpy 1.21.6 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.8 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for matplotlib==3.2.2\n","Best match: matplotlib 3.2.2\n","Adding matplotlib 3.2.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for torch==1.13.0+cu116\n","Best match: torch 1.13.0+cu116\n","Adding torch 1.13.0+cu116 to easy-install.pth file\n","Installing convert-caffe2-to-onnx script to /usr/local/bin\n","Installing convert-onnx-to-caffe2 script to /usr/local/bin\n","Installing torchrun script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for Pillow==7.1.2\n","Best match: Pillow 7.1.2\n","Adding Pillow 7.1.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for requests==2.23.0\n","Best match: requests 2.23.0\n","Adding requests 2.23.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for typing-extensions==4.4.0\n","Best match: typing-extensions 4.4.0\n","Adding typing-extensions 4.4.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for python-dateutil==2.8.2\n","Best match: python-dateutil 2.8.2\n","Adding python-dateutil 2.8.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for pyparsing==3.0.9\n","Best match: pyparsing 3.0.9\n","Adding pyparsing 3.0.9 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for cycler==0.11.0\n","Best match: cycler 0.11.0\n","Adding cycler 0.11.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for kiwisolver==1.4.4\n","Best match: kiwisolver 1.4.4\n","Adding kiwisolver 1.4.4 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for urllib3==1.24.3\n","Best match: urllib3 1.24.3\n","Adding urllib3 1.24.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for chardet==3.0.4\n","Best match: chardet 3.0.4\n","Adding chardet 3.0.4 to easy-install.pth file\n","Installing chardetect script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for idna==2.10\n","Best match: idna 2.10\n","Adding idna 2.10 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for certifi==2022.12.7\n","Best match: certifi 2022.12.7\n","Adding certifi 2022.12.7 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Searching for six==1.15.0\n","Best match: six 1.15.0\n","Adding six 1.15.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.8/dist-packages\n","Finished processing dependencies for spikingjelly==0.0.0.0.13\n"]}]},{"cell_type":"markdown","source":["###Dirs"],"metadata":{"id":"IfzpK0gl2TJ4"}},{"cell_type":"code","source":["!mkdir \"/content/runs\"\n","!mkdir \"/content/emissions\"\n","!mkdir \"/content/saved_models\"\n","!mkdir \"/content/datasets\"\n","\n","RUNS_PATH = \"/content/runs\"\n","EMISSION_PATH = \"/content/emissions\"\n","MODEL_PATH = \"/content/saved_models\"\n","DATASET_PATH = \"/content/datasets\"\n","\n","tracker = codecarbon.EmissionsTracker(output_dir = EMISSION_PATH, save_to_file=True)\n","tracker_inf = codecarbon.EmissionsTracker(output_dir = EMISSION_PATH, save_to_file=True, output_file='infe_emissions.csv')"],"metadata":{"id":"Pk9o0qvJ2VCU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672250750410,"user_tz":-60,"elapsed":5722,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"cd6c0204-4e8b-4cc2-aafa-8934ca8e66b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[codecarbon INFO @ 18:05:44] [setup] RAM Tracking...\n","[codecarbon INFO @ 18:05:44] [setup] GPU Tracking...\n","[codecarbon INFO @ 18:05:44] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 18:05:44] [setup] CPU Tracking...\n","[codecarbon WARNING @ 18:05:44] No CPU tracking mode found. Falling back on CPU constant mode.\n","[codecarbon WARNING @ 18:05:47] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n","[codecarbon INFO @ 18:05:47] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n","[codecarbon INFO @ 18:05:47] >>> Tracker's metadata:\n","[codecarbon INFO @ 18:05:47]   Platform system: Linux-5.10.133+-x86_64-with-glibc2.27\n","[codecarbon INFO @ 18:05:47]   Python version: 3.8.16\n","[codecarbon INFO @ 18:05:47]   Available RAM : 12.681 GB\n","[codecarbon INFO @ 18:05:47]   CPU count: 2\n","[codecarbon INFO @ 18:05:47]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n","[codecarbon INFO @ 18:05:47]   GPU count: 1\n","[codecarbon INFO @ 18:05:47]   GPU model: 1 x Tesla T4\n","[codecarbon INFO @ 18:05:47] [setup] RAM Tracking...\n","[codecarbon INFO @ 18:05:47] [setup] GPU Tracking...\n","[codecarbon INFO @ 18:05:47] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 18:05:47] [setup] CPU Tracking...\n","[codecarbon WARNING @ 18:05:47] No CPU tracking mode found. Falling back on CPU constant mode.\n","[codecarbon WARNING @ 18:05:49] We saw that you have a Intel(R) Xeon(R) CPU @ 2.30GHz but we don't know it. Please contact us.\n","[codecarbon INFO @ 18:05:49] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.30GHz\n","[codecarbon INFO @ 18:05:49] >>> Tracker's metadata:\n","[codecarbon INFO @ 18:05:49]   Platform system: Linux-5.10.133+-x86_64-with-glibc2.27\n","[codecarbon INFO @ 18:05:49]   Python version: 3.8.16\n","[codecarbon INFO @ 18:05:49]   Available RAM : 12.681 GB\n","[codecarbon INFO @ 18:05:49]   CPU count: 2\n","[codecarbon INFO @ 18:05:49]   CPU model: Intel(R) Xeon(R) CPU @ 2.30GHz\n","[codecarbon INFO @ 18:05:49]   GPU count: 1\n","[codecarbon INFO @ 18:05:49]   GPU model: 1 x Tesla T4\n"]}]},{"cell_type":"markdown","source":["#Datasets"],"metadata":{"id":"ttxzaQzN2izX"}},{"cell_type":"code","source":["def init_data_loader(dataset, batch, shuffle:bool=True): \n","\n","  data_loader = data.DataLoader(\n","            dataset = dataset,\n","            batch_size = batch,\n","            shuffle = shuffle,\n","            drop_last = True)  \n","  \n","  return data_loader\n","\n","\n","\n","def transformations (steps):\n","\n","  crop = steps.crop_size[0]\n","  resize_s = steps.resize_size[0]\n","  interpolation = steps.interpolation\n","  \n","  compose = torchvision.transforms.Compose([\n","      torchvision.transforms.CenterCrop(crop),\n","      torchvision.transforms.Resize(resize_s, interpolation = interpolation),\n","      torchvision.transforms.ToTensor()\n","  ])\n","\n","  return compose"],"metadata":{"id":"mXyb2Npw3Rjm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Mnist\n"],"metadata":{"id":"sPraYV81ZdXc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQG8QcT69rgr"},"outputs":[],"source":["train_dataset = torchvision.datasets.MNIST(\n","        root = DATASET_PATH + \"/\" + \"MNIST/train\",\n","        train = True,\n","        transform = torchvision.transforms.ToTensor(),\n","        download = True)\n","\n","test_dataset = torchvision.datasets.MNIST(\n","        root = DATASET_PATH + \"/\" + \"MNIST/test\",\n","        train = False,\n","        transform = torchvision.transforms.ToTensor(),\n","        download = True)\n","\n","#Split train -> train and validation\n","train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])"]},{"cell_type":"markdown","source":["###ImgNet"],"metadata":{"id":"Jl3K9ujaZftc"}},{"cell_type":"code","source":["drive.mount('/content/drive', force_remount=True)\n","MAIN_PATH = '/content/drive/MyDrive/UNIPD_Projects/Decepticon'\n","\n","IMGNET_PATH = '/content/datasets/ImgNet'\n","!mkdir '/content/datasets/ImgNet'\n","\n","shutil.copyfile(MAIN_PATH + '/test_imgs.zip', IMGNET_PATH + '/test_imgs.zip')\n","\n","!mkdir '/content/datasets/ImgNet/imgs'\n","!unzip '/content/datasets/ImgNet/test_imgs.zip' -d  '/content/datasets/ImgNet/imgs'\n","\n","IMGNET_PATH = '/content/datasets/ImgNet/imgs/test'\n","\n","lis = pd.read_pickle(r'/content/drive/MyDrive/UNIPD_Projects/Decepticon/lab_dict')\n","\n","with open(os.path.join(MAIN_PATH, 'label_dict.pkl'), 'rb') as f:\n","    lab_name_n = pickle.load(f)"],"metadata":{"id":"QIfAraI3ZWyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_std_pretrained(model_name):\n","\n","  model = tv_models.get_model(model_name, weights=\"DEFAULT\")\n","  model_weights = tv_models.get_model_weights(model_name)\n","  preprocess_steps = model_weights.DEFAULT.transforms()\n","\n","  model_compose = transformations(preprocess_steps)\n","\n","  return model, model_compose"],"metadata":{"id":"ueyHJWZ-UjOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, compose_vgg19 = get_std_pretrained('vgg19_bn')\n","_, compose_resnet50 = get_std_pretrained('resnet50')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["9990778e40dc477eb16a02fc454b642e","b0cee7595271464a958702a0410ff32f","42373dc2764140aa8e3928aa04ff66d7","60f90ca8ed7e49249c7c0cc4f37cf4ca","9a7eb5e671694921a0a3135959d717a1","d1128ee2e0b84377acda4bc56a1b76fa","bce48006fa834c378594785bffb64341","51c24de1000942c2a350697f587d59b9","e99086bba46349469d623d5142d9d74f","1f8108a1fa8c45169bf296853751fbfe","31b2ae532553487bb03b1a7794b4f8ca"]},"id":"JkxmW0srUqGM","executionInfo":{"status":"ok","timestamp":1672256898438,"user_tz":-60,"elapsed":3444,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"7f891afb-6fd8-4913-f517-3d112c6b0712"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9990778e40dc477eb16a02fc454b642e"}},"metadata":{}}]},{"cell_type":"code","source":["# Download ImageNet labels\n","!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","\n","# Read the categories\n","with open(\"imagenet_classes.txt\", \"r\") as f:\n","    imgnet_categories = [s.strip() for s in f.readlines()]"],"metadata":{"id":"_6vKPhFHaHLD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672256898788,"user_tz":-60,"elapsed":353,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"3206275e-de49-46a4-9879-94926c638609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-28 19:48:18--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10472 (10K) [text/plain]\n","Saving to: ‘imagenet_classes.txt’\n","\n","\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n","\n","2022-12-28 19:48:18 (84.6 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n","\n"]}]},{"cell_type":"code","source":["imgnet_test = []\n","imgnet_test_names = []\n","\n","for root, __, files in os.walk(IMGNET_PATH):\n","  for f in files:\n","    imgnet_test.append(Image.open(os.path.join(root, f)))\n","    imgnet_test_names.append(int(f.replace('.png', '')))\n","\n","imgnet_test_names_sorted, imgnet_test_sorted = zip(*sorted(zip(imgnet_test_names, imgnet_test)))"],"metadata":{"id":"YhuFpiEwaxUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls_vgg19 = []\n","ls_resnet50 = []\n","\n","for a, b in zip(imgnet_test_sorted, imgnet_test_names_sorted):\n","  a_vgg19 = compose_vgg19(a)\n","  a_resnet50 = compose_resnet50(a)\n","\n","  ls_vgg19.append(tuple((a_vgg19,b)))\n","  ls_resnet50.append(tuple((a_resnet50,b)))\n"],"metadata":{"id":"k5fDQxd6hF56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","imgnet_vgg19_loader = init_data_loader(ls_vgg19, batch_size)\n","imgnet_resnet50_loader = init_data_loader(ls_resnet50, batch_size)"],"metadata":{"id":"OKdfxUPnlV5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Cifar10"],"metadata":{"id":"wImeKmPU3ASG"}},{"cell_type":"code","source":["#torch.Size([3, 32, 32])\n","cifar_compose = torchvision.transforms.Compose([\n","                    torchvision.transforms.Resize((32,32)),\n","                    torchvision.transforms.ToTensor(),\n","                    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","train_cifar = torchvision.datasets.CIFAR10(\n","                  root = DATASET_PATH + \"/\" + \"Cifar10/train\",\n","                  train = True,\n","                  download = True, \n","                  transform = cifar_compose)\n","\n","test_cifar = torchvision.datasets.CIFAR10(\n","                  root = DATASET_PATH + \"/\" + \"Cifar10/test\",\n","                  train = False,\n","                  download = True, \n","                  transform = cifar_compose)\n","\n","#Split train -> train and validation\n","train_cifar, val_cifar = torch.utils.data.random_split(train_cifar, [.9, .1])\n","\n","cifar_classes = ('plane', 'car', 'bird', 'cat',\n","                  'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","cifar_nclasses = len(cifar_classes)"],"metadata":{"id":"WFbSrwfa3CdJ","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["ebf8dce3218f42759fd58f66a390daee","573014846e2b44bcbc9942c31039acb5","5cb2f1d6e3b7474aab0e71ffe7c5841b","2bc3f0f20409417e80880ccb951eb4fe","725be2034d5e482f888e3165ceab0a49","af2c352486b843b089f406e3319ff22d","292e4e9ef5c349429ec01c56385623d8","25ff4b407ab3490bb0c642979c4072c7","c5c06f4a217c4413bf9679d89b1996fe","b5fe04abcea04e34936dd57bccf31590","e2240e934e644c7d94429332f8fbd69f","69a5db5404c543d8afd5d820ce43d94d","0c1919534bd94519b07371b876b563d5","5207dc8de52e414b87e2c68d7c6ddbec","a6e6790a424a4a26a9cbaa8f07c83650","7d323832f25441eaa0aef3a7754473f7","0a11302aefb845f99b5fed100c6708a2","c9f6c0efcae446b98241c93eab1f11ae","19f191415c0a437b9094d261125b423e","891ad5174dc0486da93a36736a64f9a5","987c285e80ae45e6beb10332055f1bbf","fcc5368331424993a4c2f6fa2d38018b"]},"executionInfo":{"status":"ok","timestamp":1672250763358,"user_tz":-60,"elapsed":12952,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"d2f3bed8-5716-4631-f211-1029a74d0787"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/datasets/Cifar10/train/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf8dce3218f42759fd58f66a390daee"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting /content/datasets/Cifar10/train/cifar-10-python.tar.gz to /content/datasets/Cifar10/train\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/datasets/Cifar10/test/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a5db5404c543d8afd5d820ce43d94d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting /content/datasets/Cifar10/test/cifar-10-python.tar.gz to /content/datasets/Cifar10/test\n"]}]},{"cell_type":"markdown","source":["#Functions"],"metadata":{"id":"83oXeFkD3iWj"}},{"cell_type":"markdown","source":["###Utils functions"],"metadata":{"id":"NSffkyLU4R24"}},{"cell_type":"code","source":["def tensor_to_PIL (tens):\n","  return torchvision.transforms.ToPILImage()(tens)"],"metadata":{"id":"g_lqJ4_34Txa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_performance_dict(hist):\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n","    axes[0].plot(hist['Train_acc'], label='train acc')\n","    axes[0].plot(hist['Valid_acc'], label='valid acc')\n","    axes[0].title.set_text('Accuracy train and val')\n","    axes[0].legend()\n","\n","    axes[1].plot(hist['Train_loss'], label='train loss')\n","    axes[1].plot(hist['Valid_loss'], label='valid loss')\n","    axes[1].title.set_text('Loss train and val')\n","    axes[1].legend()\n","\n","    plt.show()\n","\n","\n","def plot_performance_list(acc, loss):\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n","    axes[0].plot(acc, label='train acc')\n","    axes[0].title.set_text('Accuracy train')\n","\n","    axes[1].plot(loss, label='train loss')\n","    axes[1].title.set_text('Loss train')\n","\n","    plt.show()"],"metadata":{"id":"v4D7A7t04X7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def single_infer_topk(model, img, compose, idx_label, k=5, plot_img = False):\n","\n","  model.to(device)\n","  model.eval()\n","\n","  with torch.no_grad():\n","    tens_img = compose(img).to(device)\n","    out_fr = model(tens_img.unsqueeze(0))\n","\n","  probabilities = torch.nn.functional.softmax(out_fr[0], dim=0)\n","  top5_prob, top5_catid = torch.topk(probabilities, k)\n","\n","  print(\"GROUND TRUTH: \", lab_name_n(idx_label), idx_label)\n","  \n","  for i in range(top5_prob.size(0)):\n","      print(imgnet_categories[top5_catid[i]], np.round(top5_prob[i].item(),3))\n","  \n","  if plot_img == True:\n","    plt.imshow(img)\n","\n","  if(\"spiking\" in str(type(model)).lower()):\n","    functional.reset_net(model)\n","    print(\"\\n -- resetted net --\")"],"metadata":{"id":"jggkT-fc7HzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Spiking functions"],"metadata":{"id":"uZ7JcMp53qJJ"}},{"cell_type":"code","source":["def fit_and_eval(net, epochs, optimizer, criterion, num_classes, \n","                 train_data_loader, validation_data_loader, device,\n","                 T, validation_step = False, convolution = True, onehot = False):\n","  \n","  writer = SummaryWriter()\n","\n","  dict_hist = {\n","      'Train_acc' : [],\n","      'Train_loss' : [],\n","      'Valid_acc' : [],\n","      'Valid_loss': [],\n","      'Time_train': []\n","  }\n","  \n","  # Gradient scaling helps prevent gradients with small magnitudes from \n","  # flushing to zero (“underflowing”) when training with mixed precision.\n","  scaler = amp.GradScaler()\n","\n","  if scaler is not None: \n","    functional.reset_net(net)\n","    net.train().to(device)\n","\n","    for epoch in range(epochs):\n","        print(\"Epoch {}:\".format(epoch+1))\n","\n","        # ------------- TRAIN SET ----------- # \n","        train_loss = 0.\n","        train_acc = 0.\n","        train_samples = 0\n","\n","        time_train_batch = timing.time()\n","\n","        for img, label in load_bar(train_data_loader):\n","            optimizer.zero_grad()\n","            img = img.to(device)\n","            label = label.to(device)\n","\n","            if onehot == True:\n","              label = F.one_hot(label, num_classes).float()\n","\n","            with amp.autocast():\n","              if(hasattr(net, 'T') == False):\n","                out_fr = 0.\n","                for t in range(T):\n","                    if (convolution == False):\n","                      img = encoder(img)\n","                    out_fr += net(img)\n","                out_fr = out_fr / T   \n","              else:\n","                  if (convolution == False):\n","                    img = encoder(img)\n","                  out_fr = net(img)\n","\n","              loss = criterion(out_fr, label)\n","              scaler.scale(loss).backward()\n","              scaler.step(optimizer)\n","              scaler.update()\n","            \n","            train_samples += label.numel()\n","            train_loss += loss.item() * label.numel()\n","            train_acc += (out_fr.argmax(1) == label).float().sum().item()\n","\n","            functional.reset_net(net)\n","\n","        time_end = timing.time()\n","\n","        train_loss /= train_samples\n","        train_acc /= train_samples\n","\n","        dict_hist['Train_acc'].append(train_acc)\n","        dict_hist['Train_loss'].append(train_loss)\n","        dict_hist['Time_train'].append(time_end - time_train_batch)\n","\n","        # ------------- STORING TRAIN BATCH RESULTS ----------- # \n","        writer.add_scalar('Loss/train', train_loss, epoch)\n","        writer.add_scalar('Accuracy/train', train_acc, epoch)\n","\n","        writer.flush()\n","        writer.close()\n","       \n","\n","        # ------------- VALIDATION ----------- # \n","        if(validation_step == True):\n","          net.eval()\n","          val_loss = 0.\n","          val_acc = 0.\n","          val_samples = 0\n","\n","          with torch.no_grad():\n","              for img, label in validation_data_loader:\n","                  img = img.to(device)\n","                  label = label.to(device)\n","\n","                  if onehot == True:\n","                    label = F.one_hot(label, num_classes).float()\n","\n","                  with amp.autocast():\n","                    if(hasattr(net, 'T') == False):\n","                      out_fr = 0.\n","                      for t in range(T):\n","                          if (convolution == False):\n","                            img = encoder(img)\n","                          out_fr += net(img)\n","                      out_fr = out_fr / T   \n","                    else:\n","                        if (convolution == False):\n","                          img = encoder(img)\n","                        out_fr = net(img)\n","                    \n","                  loss = criterion(out_fr, label)\n","                  val_samples += label.numel()\n","                  val_loss += loss.item() * label.numel()\n","                  val_acc += (out_fr.argmax(1) == label).float().sum().item()\n","\n","                  functional.reset_net(net)\n","\n","          val_loss /= val_samples\n","          val_acc /= val_samples\n","\n","          dict_hist['Valid_acc'].append(val_acc)\n","          dict_hist['Valid_loss'].append(val_loss)\n","\n","          # ------------- STORING VALID BATCH RESULTS ----------- # \n","          writer.add_scalar(RUNS_PATH + '/Loss/val', val_loss, epoch)\n","          writer.add_scalar(RUNS_PATH + '/Accuracy/val', val_acc, epoch)\n","\n","          writer.flush()\n","          writer.close()\n","\n","        if(validation_step == True):\n","          print(\"B train acc: {:.4f} -- B train Loss: {:.4f} ## B valid acc: {:.4f} -- B valid loss: {:.4f}  ## Time: {:.2f}\".format(train_acc, train_loss, val_acc, val_loss, (time_end - time_train_batch)))\n","        else:\n","          print(\"B train acc: {:.4f} -- B train Loss: {:.4f} ## Time: {:.2f}\".format(train_acc, train_loss, (time_end - time_train_batch)))\n","\n","    return dict_hist\n","\n","  else:\n","    return print(\"Error in Scaler -> None\")\n"],"metadata":{"id":"H1r3dUL33uUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def val(net, device, data_loader, T=None):\n","    net.eval().to(device)\n","    correct = 0.0\n","    total = 0.0\n","    with torch.no_grad():\n","        for batch, (img, label) in enumerate(tqdm(data_loader)):\n","            img = img.to(device)\n","            if T is None:\n","                out = net(img)\n","            else:\n","                for m in net.modules():\n","                    if hasattr(m, 'reset'):\n","                        m.reset()\n","                for t in range(T):\n","                    if t == 0:\n","                        out = net(img)\n","                    else:\n","                        out += net(img)\n","            correct += (out.argmax(dim=1) == label.to(device)).float().sum().item()\n","            total += out.shape[0]\n","        acc = correct / total\n","        print('Validating Accuracy: %.3f' % (acc))\n","    return acc"],"metadata":{"id":"cvRhALm3VR9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_snn(net, device, data_loader, T):\n","  \n","    net.eval().to(device)\n","\n","    correct = 0.\n","    total = 0.\n","\n","    with torch.no_grad():\n","        for (img, label) in load_bar(data_loader):\n","            img = img.to(device)\n","            out = 0.\n","            for t in range(T):\n","                    out += net(img)\n","                    correct += (out.argmax(dim=1) == label.to(device)).float().sum().item()\n","            total += out.shape[0]\n","\n","            functional.reset_net(net)\n","\n","    print(\"\\nModel Accuracy: {:.4f}\".format((correct/total)*100))"],"metadata":{"id":"zHQ_Qlvr3xWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###ANN functions"],"metadata":{"id":"3feFcA2c6vj6"}},{"cell_type":"code","source":["def evaluate_ann(model, device, data_loader):\n","\n","  model.eval().to(device)\n","  total = 0\n","  correct = 0\n","\n","  with torch.no_grad():\n","    for batch, (img,label) in enumerate(load_bar(data_loader)):\n","      img = img.to(device)\n","      label = label.to(device)\n","\n","      output = model(img)\n","\n","      correct += (output.argmax(dim=1) == label).float().sum().item()\n","      #_ , predicted = torch.max(output.data, 1)\n","      #total += label.size(0)\n","      #correct += (predicted == label).sum().item()\n","\n","\n","    total = total + output.shape[0]\n","    \n","    print(\"\\nModel Accuracy: {:.4f}\".format((correct/total)*100))\n","    #print(f'\\nAccuracy of the network on the 10000 test images: {100 * correct // total} %')"],"metadata":{"id":"axxrSWSo6xsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Report functions"],"metadata":{"id":"IDWpn6rx5cgr"}},{"cell_type":"code","source":["def short_report(name=\"emissions.csv\"):\n","  \n","  df = pd.read_csv(EMISSION_PATH + '/' + name)\n","\n","  # Emissions: Emissions as CO₂-equivalents [CO₂eq], in kg\n","  # ._energy: Energy in kW\n","  # energy_consumed: sum of cpu+gpu+ram in kW\n","  df = df[['run_id', 'emissions', 'cpu_energy', 'gpu_energy', 'ram_energy', 'energy_consumed','Multi_step',\n","           'Time_train','Last_acc','Last_loss','Valid_acc','Valid_loss']]\n","\n","  return df"],"metadata":{"id":"idiRrdXg5erN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_report(train_dict, net, has_validation = False, name=\"emissions.csv\"):\n","  \n","  df = pd.read_csv(EMISSION_PATH + '/' + name)\n","\n","  ncols = len(df.columns)\n","  nrow = len(df.index)-1\n","  if ncols == 29:\n","    df['Multi_step'] = False\n","    df['Time_train'] = None\n","    df['Last_acc'] = None\n","    df['Last_loss'] = None\n","    df['Valid_acc'] = None\n","    df['Valid_loss'] = None\n","\n","  if(hasattr(net, 'T') == True):\n","    df.at[nrow,'Multi_step'] = True\n","  else:\n","    df.at[nrow,'Multi_step'] = False\n","\n","  tot_time = round(np.sum(train_dict['Time_train']), 2)\n","  acc_last = round(train_dict['Train_acc'][-1], 3)\n","  loss_last = round(train_dict['Train_loss'][-1], 3)\n","\n","  df.at[nrow,'Time_train'] = tot_time\n","  df.at[nrow,'Last_acc']  = acc_last\n","  df.at[nrow,'Last_loss']  = loss_last\n","\n","  if (has_validation == True):\n","      acc_last = round(train_dict['Valid_acc'][-1], 3)\n","      loss_last = round(train_dict['Valid_loss'][-1], 3)\n","\n","      df.at[nrow,'Valid_acc']  = acc_last\n","      df.at[nrow,'Valid_loss']  = loss_last\n","  else:\n","\n","      df.at[nrow,'Valid_acc']  = None\n","      df.at[nrow,'Valid_loss']  = None\n","\n","  df.to_csv(EMISSION_PATH + '/' + name)\n"],"metadata":{"id":"k2NoTwLl5hkU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Model construction"],"metadata":{"id":"_ZgGQy3fPGoY"}},{"cell_type":"code","source":["###\n","#Standard CNN\n","###\n","def first_linear_input(input_dim,dict_cnn):\n","  h_out = input_dim\n","  for i in range(len(dict_cnn['filter_size'])):\n","    h_out = np.floor((h_out-(2-1)-1)/2+1)\n","  t_d = h_out**2*dict_cnn['n_filters'][-1]\n","  return int(t_d)\n","\n","def cnn_layer_builder(model,in_f,out_f,size,pool):\n","\n","  model.append(nn.Conv2d(in_f, out_f, kernel_size=size, padding='same'))\n","  model.append(nn.BatchNorm2d(out_f))\n","  model.append(nn.ReLU())\n","  if pool == 'maxpool':\n","    model.append(nn.MaxPool2d(2,2))\n","  return model\n","\n","def lin_layer_builder(model,s_in,s_out,act='relu'):\n","  model.append(nn.Linear(s_in, s_out))\n","  if act == 'relu':\n","    model.append(nn.ReLU())\n","  elif act == 'out':\n","    pass\n","  return model\n","\n","def init_cnn(cnn_d):\n","  filter_sizes = cnn_d['filter_size']\n","  n_filters = cnn_d['n_filters']\n","\n","  l_sizes = cnn_d['linear_size']\n","  l_acts = cnn_d['lin_act']\n","\n","  model = nn.Sequential()\n","  for k,j in zip( range(len(filter_sizes)) , range(1,len(n_filters))):\n","    model = cnn_layer_builder(model,n_filters[j-1], n_filters[j], filter_sizes[k], 'maxpool')\n","  model.append(nn.Flatten())\n","  for j,i in zip( range(len(l_acts)), range(1, len(l_sizes)) ):\n","    model = lin_layer_builder(model, l_sizes[i-1], l_sizes[i], l_acts[j])\n","            \n","\n","\n","  return model"],"metadata":{"id":"o9X5qQfWPORm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###\n","#CSNN\n","###\n","\n","def first_linear_input(input_dim,dict_scnn):\n","  h_out = input_dim\n","  for p in dict_scnn['type_pool']:\n","    if p != 'no':\n","      h_out = h_out//2\n","  return dict_scnn['n_filters'][-1]*h_out**2\n","\n","def scnn_layer_builder(model,in_f,out_f,size,pool,surr_f):\n","\n","  model.append(layer.Conv2d(in_f, out_f, kernel_size=size, padding=(size//2,size//2), bias=False))\n","  model.append(layer.BatchNorm2d(out_f))\n","  if surr_f=='atan':\n","    model.append(neuron.IFNode(surrogate_function=surrogate.ATan()),)\n","  if pool == 'maxpool':\n","    model.append(layer.MaxPool2d(2,2))\n","  return model\n","\n","def slin_layer_builder(model,s_in,s_out,surr_f):\n","  model.append(layer.Linear(s_in, s_out, bias=False))\n","  if surr_f == 'atan':\n","    model.append(neuron.IFNode(surrogate_function=surrogate.ATan()),)\n","  return model"],"metadata":{"id":"oKNLpF-pPaPY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###STDP functions"],"metadata":{"id":"G3xaheLiPxWf"}},{"cell_type":"code","source":["def init_optimizers(net, istances, backprop_opt='adam', stdp_opt='sgd'):\n","  stdp_learners = []\n","  params_stdp = []\n","  params_gd = []\n","\n","  layers = tuple(net.children())[0]\n","\n","  for i in range(net.conv_fc.__len__()):\n","    if isinstance(layers[i], instances):\n","      stdp_learners.append(\n","          learning.STDPLearner(step_mode='m', synapse=net.conv_fc[i],\n","                                sn=net.conv_fc[i+1], tau_pre=2, tau_post=100,\n","                                f_pre=f_weight, f_post=f_weight)\n","      )\n","  for m in net.modules():\n","    if isinstance(m, instances):\n","      for p in m.parameters():\n","        params_stdp.append(p)\n","\n","  params_stdp_set = set(params_stdp)\n","  params_gradient_descent = []\n","  for p in net.parameters():\n","    if p not in params_stdp_set:\n","      params_gd.append(p)\n","\n","  if backprop_opt == 'adam':\n","    optimizer_gd = torch.optim.Adam(params_gd, lr=lr)\n","  if stdp_opt == 'sgd':\n","    optimizer_stdp = torch.optim.SGD(params_stdp, lr=lr)\n","\n","  return optimizer_gd, optimizer_stdp, stdp_learners"],"metadata":{"id":"jua3CxR1PzpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit_stdp(net, epochs, train_data_loader, device):\n","  dict_hist = {\n","      'Train_acc' : [],\n","      'Train_loss' : [],\n","      'Valid_acc' : [],\n","      'Valid_loss': [],\n","      'Time_train': []\n","  }\n","\n","  writer = SummaryWriter('/content/runs')\n","\n","  # Gradient scaling helps prevent gradients with small magnitudes from \n","  # flushing to zero (“underflowing”) when training with mixed precision.\n","  scaler = amp.GradScaler()\n","\n","  if scaler is not None: \n","    functional.reset_net(net)\n","\n","    for epoch in range(epochs):\n","        print(\"Epoch {}:\".format(epoch+1))\n","\n","        # ------------- TRAIN SET ----------- # \n","        net.train()\n","        train_loss = 0\n","        train_acc = 0\n","        train_samples = 0\n","\n","        time_train_batch = timing.time()\n","\n","\n","        for img, label in load_bar(train_data_loader):\n","            optimizer_gd.zero_grad()\n","            optimizer_stdp.zero_grad()\n","            img = img.to(device)\n","            label = label.to(device)\n","            label_onehot = F.one_hot(label, 10).float()\n","\n","            #with amp.autocast():\n","            out_fr = net(img)\n","\n","            loss = F.mse_loss(out_fr, label_onehot)\n","            #scaler.scale(loss).backward()\n","            loss.backward()\n","\n","            optimizer_stdp.zero_grad()\n","\n","            for i in range(stdp_learners.__len__()):\n","              stdp_learners[i].step(on_grad=True)\n","\n","            #scaler.step(optimizer_gd)\n","            #scaler.step(optimizer_stdp)\n","            #scaler.update()\n","            optimizer_gd.step()\n","            optimizer_stdp.step()\n","\n","            train_samples += label.numel()\n","            train_loss += loss.item() * label.numel()\n","            train_acc += (out_fr.argmax(1) == label).float().sum().item()\n","\n","            functional.reset_net(net)\n","            del loss\n","\n","        time_end = timing.time()\n","\n","        train_loss /= train_samples\n","        train_acc /= train_samples\n","\n","        dict_hist['Train_acc'].append(train_acc)\n","        dict_hist['Train_loss'].append(train_loss)\n","        dict_hist['Time_train'].append(time_end - time_train_batch)\n","\n","        # ------------- STORING TRAIN BATCH RESULTS ----------- # \n","        writer.add_scalar('Loss/train', train_loss, epoch)\n","        writer.add_scalar('Accuracy/train', train_acc, epoch)\n","        writer.flush()\n","        writer.close()\n","        print()\n","        print(\"B train acc: {:.4f} -- B train Loss: {:.4f} ## Time: {:.2f}\".format(train_acc, train_loss, (time_end - time_train_batch)))\n","\n","  return dict_hist"],"metadata":{"id":"nDBwHhkeP2Z0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Classes"],"metadata":{"id":"9vVvz2vD5mhG"}},{"cell_type":"code","source":["class SNN(nn.Module):\n","\n","  def __init__(self, **kwargs):\n","      super().__init__()\n","\n","      self.layer = nn.Sequential(\n","          layer.Flatten(),\n","          layer.Linear(28 * 28, 10, bias=False),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()))\n","      \n","      if(len(kwargs) > 0):\n","        self.T = kwargs['T']\n","        functional.set_step_mode(self, step_mode=\"m\")\n","        functional.set_backend(self, backend=\"cupy\")\n","\n","  def forward(self, x:torch.Tensor):\n","    if(hasattr(self, 'T')):\n","      x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n","      x_seq = self.layer(x_seq)\n","      out = x_seq.mean(0)\n","    else:\n","      out = self.layer(x)\n","    return out"],"metadata":{"id":"oKqEwOrY5o_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CSNN_cifar(nn.Module):\n","\n","    def __init__(self, T: int, channels: int):\n","        super().__init__()\n","        self.T = T\n","\n","        # torch.Size([3, 32, 32])\n","        self.conv_fc = nn.Sequential(\n","            \n","          #the image-spike encoding is implemented by the first three layers of the network\n","          layer.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n","          layer.BatchNorm2d(channels),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()),  \n","\n","          layer.Conv2d(channels, 16, kernel_size=3, padding=1,bias=False),\n","          layer.Conv2d(16, 16, kernel_size=3, padding=1,bias=False),\n","          layer.BatchNorm2d(16),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()),\n","\n","          layer.Conv2d(16, 32, kernel_size=3, padding=1,bias=False),\n","          layer.Conv2d(32, 32, kernel_size=3, padding=1,bias=False),\n","          layer.BatchNorm2d(32),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()),\n","          layer.AvgPool2d(2,2), #32 -> 16\n","          \n","          layer.Flatten(),\n","          layer.Linear(32 * 16 * 16, 512, bias=False),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()),\n","\n","          layer.Linear(512, 128, bias=False),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()),\n","\n","          layer.Linear(128, 10, bias=False),\n","          neuron.IFNode(surrogate_function=surrogate.ATan()))\n","  \n","        functional.set_step_mode(self, step_mode=\"m\")\n","        functional.set_backend(self, backend=\"cupy\")\n","\n","    def forward(self, x: torch.Tensor):\n","      x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)  # [N, C, H, W] -> [T, N, C, H, W]\n","      x_seq = self.conv_fc(x_seq)\n","      fr = x_seq.mean(0)\n","      return fr\n"],"metadata":{"id":"foKYDrPr5r8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CSNN_d(nn.Module):\n","\n","    def __init__(self, dict_snn, backend=\"cupy\",):\n","      super().__init__()\n","      self.T = dict_snn['T']\n","      self.conv_fc = self.build_net(dict_snn)\n","\n","      functional.set_step_mode(self, step_mode=\"m\")\n","      functional.set_backend(self, backend=\"cupy\")\n","        \n","    def build_net(self,dict_snn):\n","      model = nn.Sequential()\n","      for k,j in zip( range(len(dict_snn['filter_size'])) , range(1,len(dict_snn['n_filters']))):\n","        model = scnn_layer_builder(model, dict_snn['n_filters'][j-1], dict_snn['n_filters'][j], dict_snn['filter_size'][k], dict_snn['type_pool'][k],dict_snn['surrogate'])\n","      model.append(layer.Flatten())\n","      for i in range(1, len(dict_snn['linear_size'])):\n","        model = slin_layer_builder(model, dict_snn['linear_size'][i-1], dict_snn['linear_size'][i],dict_snn['surrogate'])\n","\n","      return model\n","\n","    def forward(self, x: torch.Tensor):\n","      x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)  # [N, C, H, W] -> [T, N, C, H, W]\n","      x_seq = self.conv_fc(x_seq)\n","      fr = x_seq.mean(0)\n","      return fr"],"metadata":{"id":"Yb13zOxkPoYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ANN to SNN convertion"],"metadata":{"id":"bxh7Vgst6F66"}},{"cell_type":"code","source":["batch_size = 128\n","train_data_loader = init_data_loader(train_dataset, batch_size)\n","\n","model_converter = ann2snn.Converter(mode='max', dataloader = train_data_loader)\n","snn_model = model_converter(cnn)"],"metadata":{"id":"TdjtWrqA6V4R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data_loader = init_data_loader(test_dataset, batch_size)"],"metadata":{"id":"_3fMJixT6iNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["T = 50\n","mode_max_accs = evaluate_snn(snn_model, device, test_data_loader, T=T)\n","print('SNN accuracy (simulation %d time-steps): %.4f' % (T, mode_max_accs[-1]))"],"metadata":{"id":"U3vPfJll6imy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Pretrained models"],"metadata":{"id":"DE8BCewn7NGs"}},{"cell_type":"code","source":["all_models = tv_models.list_models()\n","classification_models = tv_models.list_models(module=tv_models)"],"metadata":{"id":"TxxyEcb97QTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_ls = []\n","resnet_ls = []\n","\n","for i in classification_models:\n","  if \"vgg\" in i:\n","    vgg_ls.append(i)\n","  if \"resnet\" in i:\n","    resnet_ls.append(i)"],"metadata":{"id":"B5D3kTTb_7-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynL3toPaA7tx","executionInfo":{"status":"ok","timestamp":1672218922858,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nicola Cassetta","userId":"04262688367948738190"}},"outputId":"ec36c668-af8e-4595-b726-9428b3955f08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['vgg11',\n"," 'vgg11_bn',\n"," 'vgg13',\n"," 'vgg13_bn',\n"," 'vgg16',\n"," 'vgg16_bn',\n"," 'vgg19',\n"," 'vgg19_bn']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["s_vgg19 = spiking_vgg.spiking_vgg19_bn(pretrained=True,\n","                                      spiking_neuron=neuron.IFNode, \n","                                      surrogate_function=surrogate.ATan())\n","\n","s_resnet50 = spiking_resnet.spiking_resnet50(pretrained=True,\n","                                          spiking_neuron=neuron.IFNode, \n","                                          surrogate_function=surrogate.ATan())\n","\n","s_vgg19.to(device)\n","s_resnet50.to(device)"],"metadata":{"id":"W-4nxC25EJzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res = evaluate_snn(s_vgg19, device, imgnet_vgg19_loader, T = 50)"],"metadata":{"id":"wHJujMUKGi13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672258134401,"user_tz":-60,"elapsed":211217,"user":{"displayName":"Nicola Cassetta","userId":"09580852809237958406"}},"outputId":"9cccf6ab-3a50-4059-915e-3da4083851d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [03:30<00:00, 14.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Validating Accuracy: 0.000\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["#TEST"],"metadata":{"id":"QFk_PbxXWbsb"}},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"76W7XeQ81WJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spike_net_cifar = CSNN_cifar(T=20, channels=3)"],"metadata":{"id":"PP20yROsWmwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer = SummaryWriter()\n","\n","epochs = 30\n","batch_size = 128\n","lr = 1e-2\n","\n","# Use Adam optimizer\n","optimizer = torch.optim.Adam(spike_net_cifar.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Init data loaders\n","train_data_loader_cifar = init_data_loader(train_cifar, batch_size)\n","val_data_loader_cifar = init_data_loader(val_cifar, batch_size)\n","\n","train_data_loader_cifar.num_workers=2"],"metadata":{"id":"Q0zsPtZkWdzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","def fit_and_eval(net, epochs, optimizer, criterion, num_classes, \n","                 train_data_loader, validation_data_loader, device,\n","                 T, validation_step = False, convolution = True, onehot = False):\n","'''\n","\n","train_val_ls = fit_and_eval(spike_net_cifar, epochs, optimizer, criterion, cifar_nclasses,\n","                            train_data_loader_cifar, val_data_loader_cifar, device, \n","                            T = spike_net_cifar.T, validation_step = False, convolution = True, onehot=False)"],"metadata":{"id":"A0B9v7N5YoiS"},"execution_count":null,"outputs":[]}]}